{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqXV9DA9Mb_I"
      },
      "source": [
        "## Project 4 - Retrieval-augmented generation\n",
        "The task is to create a Retrieval-Augmented Generation (RAG) system using an embedding model, a vector store, and a local LLM (Large Language Model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8svh7xkXPu0c"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community\n",
        "!pip install --upgrade --quiet langchain-huggingface text-generation datasets\n",
        "!pip install --upgrade --quiet  langchain langchain-core langchain-community langchain-text-splitters langchain-milvus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitZQlpGNDEt"
      },
      "source": [
        "API Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyqPQHChPu0e",
        "outputId": "5d31e775-d5b1-4cea-bdfb-c5e9d294f8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangChain API Key··········\n",
            "Enter your HuggingFace token: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter LangChain API Key\")\n",
        "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace token: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtOQ5cIwNGSX"
      },
      "source": [
        "#### Create local LLM endpoint\n",
        "For this purpose Llama 3.2 3B was chosen as it can be easily handled by Google Collab T4 Machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QneJEjYgPu0f",
        "outputId": "7c029b77-cd8d-4dcb-8b09-d411aa50cb4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "llm = ChatHuggingFace(llm=endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HChGhuNPNczb"
      },
      "source": [
        "#### Embeddings\n",
        "Embeddings model used in this project is stella_en_1.5B as it is lightweight and other languages than english are not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIPPdTZbPu0f"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_1.5B_v5\", model_kwargs={\"trust_remote_code\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbKbI21hNxS5"
      },
      "source": [
        "#### Load and split document\n",
        "As a test input Bee Movie scenario will be used. It is loaded from github page, and splitted into chunks with 500 length and 250 overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y31_MwoDPu0g"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://gist.github.com/MattIPv4/045239bc27b16b2bcf7a3a9a4648c08a\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            id=\"file-bee-movie-script\"\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=250)\n",
        "all_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLg1E1nbOWd3"
      },
      "source": [
        "#### Vector store\n",
        "Milvus, an open-surce vector store, is used not only because its price (free) but also it's high-performance and scalability. Here we initialize database and load document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhi1htU-Pu0g",
        "outputId": "fcaf2479-1aa3-425a-b6d0-d213d779a8de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-08 13:59:30,152 [ERROR][handler]: RPC error: [create_index], <MilvusException: (code=65535, message=invalid index type: HNSW, local mode only support FLAT IVF_FLAT AUTOINDEX: )>, <Time:{'RPC start': '2025-01-08 13:59:30.150977', 'RPC error': '2025-01-08 13:59:30.152357'}> (decorators.py:140)\n"
          ]
        }
      ],
      "source": [
        "from langchain_milvus import Milvus, Zilliz\n",
        "\n",
        "vector_store = Milvus.from_documents(\n",
        "    documents=all_splits,\n",
        "    embedding=embeddings,\n",
        "    connection_args={\n",
        "        \"uri\": \"./milvus_demo.db\",\n",
        "    },\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vzGUm8ZROw-"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ItwRlVPsvJ"
      },
      "source": [
        "#### Graph\n",
        "Using langGraph a graph combining all the elements of RAG is created. Then some test questions are asked to check if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjpJD5cUPu0g"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbQmOiDYPu0g",
        "outputId": "beb00bde-9d6d-4254-d3bb-46df8e8adb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Donkey needed to find a hive, and he needed to stop a fly. However, before that, Donkey had to fill the seat of the Flower Cart with 4 Roses. Roses were flowers.\n"
          ]
        }
      ],
      "source": [
        "response = graph.invoke({\"question\": \"What flower did donkey need to find?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoqQ1_KhJMO-",
        "outputId": "15935320-ffd4-4919-b7e9-b00175b11914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The workers at Honey Farms want the Thomas 3000 bee smoker, as seen in the context of the \"smoking gun\". The Thomas 3000 is a semi-automatic smoker that produces a consistent flow of smoke, knocking bees out more efficiently. It is described as having ninety puffs a minute and containing twice the nicotine and tar of other smokers.\n"
          ]
        }
      ],
      "source": [
        "response = graph.invoke({\"question\": \"What is the name of the bee smoker that workers at Honey Farms want?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz4GIrLPJMzY",
        "outputId": "de584d6a-755b-4061-e296-4e76fca338dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's not explicitly stated in the context, but it seems that no one, including mosquitoes, likes mosquitoes, and they are often met with hostility (\"Just smack,\" \"They just smack\").\n"
          ]
        }
      ],
      "source": [
        "response = graph.invoke({\"question\": \"Who likes Mosquito girls?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP34tQqjTwII",
        "outputId": "49cc8711-438c-400c-aaea-4b1375737715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The boyfriend's name is Mr. Benson Bee (or Barry Bee to those close to him). However, it is later confirmed that he is actually his bee parent's son, due to royal bee anatomy. His name is her human boyfriend, Mr. Benson, but a local bee or companion, named Barry, in the Hive\n"
          ]
        }
      ],
      "source": [
        "response = graph.invoke({\"question\": \"Whats the name of the boyfriend?\"})\n",
        "print(response[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
